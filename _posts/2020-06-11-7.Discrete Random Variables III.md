---
title: MIT 6.041 Probabilistic Systems Analysis and Applied Probability - 7.Discrete Random Variables III
layout: post
categories: [Probability, Math, MIT 6.041 Probabilistic Systems Analysis and Applied Probability]
image: /assets/img/MIT6.041/cover.jpg
description: "MIT 6.041 7.Discrete Random Variables III"
customexcerpt: "Notes for MIT opencourse MIT 6.041 Probabilistic Systems Analysis and Applied Probability"
---

* 
{:toc}




## 7.Discrete Random Variables III 离散随机变量（三）

这课主要讲的是**多随机变量 Multiple Random Variables**下的各种Theorem以及应用

为了方便，每个大标题前的“多随机变量的”将被略去

#### Joint PMF 联合PMF

很自然地，**多随机变量的联合PMF**如下表示

$$
p_{X_1,\cdots,X_n}(x_1,\cdots,x_n)
$$

如果我想通过多变量PMF得到其中一个随机变量的**边缘PMF marginal PMF** ，只要将对应一个值的无关随机变量的概率相加即可

$$
p_{X_k}(x_k) = \sum_{\ x_i\\i\neq k}p_{X_1,\cdots,X_n}(x_1,\cdots,x_n)
$$

**多随机变量PMF的乘法法则**如下

$$
p_{X_1,\cdots,X_n}(x_1,\cdots,x_n) = p_{X_1}(x_1)p_{X_2\|X_1}(x_2\|x_1)\cdots = \prod_{k=1}^n p_{X_k\|\bigcap_{j=1}^{k-1}X_j}(x_k\|\bigcap_{j=1}^{k-1}x_j)
$$

这无非就是乘法法则换了套符号表示，让我们回想乘法法则：

$$
P(A_1\cap\cdots\cap A_n) = P(A_1)P(A_2\|A_1)P(A_3\|A_1\cap A_2)\cdots=\prod_{k=1}^n P(A_k\|\bigcap_{j=1}^{k-1}A_j)
$$

当乘法法则中的$A_k,k=1,\cdots,n$表示的是事件"$X_k = x_k$"时，乘法法则就成了多变量PMF的乘法法则。



随机变量相互**独立 independent** ，对于所有$x_i,i=1,\cdots,n$满足

$$
p_{X_1,\cdots,X_n}(x_1,\cdots,x_n) = p_{X_1}(x_1)\cdots p_{X_n}(x_n) = \prod_{k=1}^np_{X_k}(x_k)
$$

这无非就是事件的独立性换了套符号表示，让我们回想多个事件的独立性：

$$
P\big(\bigcap_{i\in S}A_i\big) = \prod_{i\in S} P(A_i)
$$

当其中的$A_k,k=1,\cdots,n$表示的是事件"$X_k = x_k$"时，多事件的独立性就成了多变量PMF的独立性。

直观上看，就是每个随机变量取值的概率不受别的随机变量取值的影响，i.e.

$$
p_{X_k\|X_1,\cdots,X_n}(x_k\|x_1,\cdots,x_n) = p_{X_k}(x_k)
$$

**真的是这样吗？未免想的太理所当然了吧。**

![1](\assets\img\MIT6.041\7\1.jpg)

![2](\assets\img\MIT6.041\7\2.jpg)

回想多个事件的独立性，我们发现光是

$$
P(A_1\cap\cdots\cap A_n) = P(A_1)\cdots P(A_n)
$$

一个式子**是不够的**，还需要中间任意个任意事件都满足连乘式，才能说明多个事件互相独立

**为什么到了随机变量这里，就只需要一个式子了呢？**

原因就在“**对于所有**$x_i,i=1,\cdots,n$”这个条件上

因为对于所有$x_i,i=1,\cdots,n$，连乘式都成立，我们就可以将连乘式求和

$$
\sum_{x_n} p_{X_1,\cdots,X_n}(x_1,\cdots,x_n) = \sum_{x_n}p_{X_1}(x_1)\cdots p_{X_n}(x_n)\\
p_{X_1,\cdots,X_{n-1}}(x_1,\cdots,x_{n-1}) = p_{X_1}(x_1)\cdots p_{X_{n-1}}(x_{n-1})\cdot\sum_{x_n}p_{X_n}(x_n)\\
p_{X_1,\cdots,X_{n-1}}(x_1,\cdots,x_{n-1}) = p_{X_1}(x_1)\cdots p_{X_{n-1}}(x_{n-1})
$$

就得到了$X_1$到$X_n$的独立性的式子

所以，类似的，运用求和，我们就可以得出任意k个随机变量是独立的，因此**虽然只有一个式子，但是却能够说明所有随机变量相互独立**



为了有直观的理解，让我们用三个随机变量把上述内容都表示一遍

- **三随机变量联合PMF**
  
  $$
  p_{X,Y,Z}(x,y,z)
  $$

- 通过三随机变量联合PMF求X的**边际PMF marginal PMF**
  
  $$
  p_X(x) = \sum_{y,z}p_{X,Y,Z}(x,y,z)
  $$

- 三随机变量PMF的**乘法法则 multiplication rule**
  
  $$
  p_{X,Y,Z}(x,y,z) = p_X(x)p_{Y\|X}(y\|x)p_{Z\|X,Y}(z\|x,y)
  $$

- 三随机变量PMF**独立 independent** 则满足
  
  $$
  p_{X,Y,Z}(x,y,z) = p_X(x)p_Y(y)p_Z(z)
  $$
  
  **对于$x,y,z$都成立**

  对于Z求和，可得X,Y是相互独立的
  
  $$
  \sum_z p_{X,Y,Z}(x,y,z) = \sum_z p_X(x)p_Y(y)p_Z(z)\\
  \sum_z p_{X,Y,Z}(x,y,z) = p_X(x)p_Y(y) \sum_zp_Z(z)\\
  p_{X,Y}(x,y) = p_X(x)p_Y(y)
  $$
  
  同理可推出所有变量两两独立

当然对于上述的所有内容，也有加**条件 Conditional** 的版本，和不加条件的情况无区别，这里不赘述。单纯举一个条件独立性的例子：在事件A的条件下，随机变量XYZ独立，如果

$$
p_{X,Y,Z\|A}(x,y,z) = p_{X\|A}(x)p_{Y\|A}(y)p_{Z\|A}(z)
$$


#### Expectation 期望

$$
E[g(X,Y)] = \sum_x\sum_yg(x,y)p_{X,Y}(x,y)
$$

把$p_{X,Y}(x,y)$理解成$X=x,Y=y$时$g(x,y)$所占的频率/比例，就不难理解改式

和单随机变量的期望一样，在一般情况下

$$
E[g(X,Y)]\neq g(E[X],E[Y])
$$

但是对于线性函数，该式成立

此处将线性函数表示为多个变量的线性组合加常数的形式，证明很容易，不赘述

$$
E[c_1X_1+\cdots+c_nX_n+c] = c_1E[X_1]+\cdots+c_nE[X_n]+c
$$

对于**独立随机变量**，**随机变量乘积的期望等于随机变量期望的乘积**，i.e.

$$
E[XY] = E[X]E[Y]
$$

> 证明：
> 
> $$
> E[XY] = \sum_x\sum_y xyp_{X,Y}(x,y)
> \\=\sum_x\sum_y xyp_X(x)p_Y(y)
> \\=\sum_xxp_X(x)\sum_y yp_Y(y)
> \\=E[X]E[Y]
> $$

同样，对于**独立随机变量，随机变量函数的乘积的期望等于随机变量期望的函数的乘积**，i.e.

$$
E[g(X)h(Y)] = E[g(X)]E[h(Y)]
$$

这不难理解，一个独立变量的取值不会影响另一个独立变量的概率，自然不会影响另一个独立变量的函数的概率。证明过程与上相似。



#### Variance 方差

当随机变量之间独立时，方差“部分满足”线性

当$X_1\cdots X_n$相互独立时

$$
Var(X_1+\cdots+X_n) = Var(X_1)+\cdots+Var(X_n)
$$

> 我们证明n=2时的情况：
> 
> $$
> Var(X+Y) = E[(X+Y-E[X+Y])^2]\\
> =E[(X-E[X]+Y-E[Y])^2]\\
> =E[(X-E[X])^2+(Y-E[Y])^2+2(X-E[X])(Y-E[Y])]\\
> =E[(X-E[X])^2]+E[(Y-E[Y])^2]+2E[(X-E[X])(Y-E[Y])]\\
> =E[(X-E[X])^2]+E[(Y-E[Y])^2]+2E[X-E[X]]E[Y-E[Y]](independence)\\
> =E[(X-E[X])^2]+E[(Y-E[Y])^2]\\
> =Var(X)+Var(Y)
> $$
> 
> 此处$E[(X-E[X])]$等于0有两种解释
>
> 一种是从代数上
> 
> $$
> E[X-E[X]] = E[X]-E[X] = 0
> $$
> 
> 一种是从直观上，既然$E[X]$是随机变量取值的加权平均了，那么每个取值到平均/中心的距离的加权平均就一定是0

但是当独立随机变量前有系数时，方差满足的就不是线性，而是“带系数的线性”

回忆方差的性质：$Var(aX) = a^2Var(X)$

所以对于独立随机变量

$$
Var(aX+bY) = Var(aX)+Var(bY) = a^2Var(X)+b^2Var(Y)
$$


下面我们用学到的一切去解决一些问题

#### Application

#### Binomial mean & variance 二项分布的期望与方差

我们令$X = $ n次伯努利实验中成功的次数

我们知道其期望如下

$$
E[X]=\sum_{k=0}^{n} k \binom{n}{k} p^{k}(1-p)^{n-k}
$$

但是计算这个求和有够呛，所以我们用别的方法计算该式

令

$$
X_i = \begin{cases}
1\quad\text{第i次试验成功}\\
0\quad\text{第i次试验失败}
\end{cases}
$$

$X_i$的期望$E[X_i] = 1\cdot p+0\cdot(1-p) = p$


而$X$恰好是所有$X_i$之和，即

$$
X = \sum_{i=1}^n X_i
$$

所以，运用多变量期望的线性性质，我们可以得出

$$
E[X] = E[\sum_{i=1}^n X_i] = \sum_{i=1}^n E[X_i] = np
$$


$X_i$的方差$Var(X_i) = p(1-p)^2+(1-p)(0-p)^2 = p(1-p)$

同样可以用方差和期望的关系计算$Var(X_i) = E[X_i^2]-E[X_i]^2 = p-p^2 = p(1-p)$

其中$E[X_i^2] = p$是因为1的平方还是1，0的平方还是0，所以计算过程不变

因为是伯努利试验，所以每个$X_i$是互相独立的，所以$X$的方差为

$$
Var(X) = Var(\sum_{i=1}^n X_i) = \sum_{i=1}^n Var(X_i) = np(1-p)
$$

这符合直观的预测，当抛硬币的概率五五开的时候，正反分散最大，不确定性最大，方差最大；当抛一面的概率为1时，所有结果都一样，没有不确定性，方差为0



####  The hat problem 帽子问题

n个人去迪厅蹦迪，他们每个人都有一顶一模一样的帽子，蹦迪前他们把帽子都放在了一个地方，蹦完迪他们出来拿帽子，由于都是一模一样的，就随便拿，完全随机。问他们中恰好拿到自己帽子的人（我愿称之为欧皇）的个数的期望和方差为何？

我们用和上题一样的trick，令

$$
X_i = \begin{cases}
1\quad\text{第i个人拿到了他自己的帽子，事欧皇}\\
0\quad\text{第i个人拿的别人的帽子，事非酋}
\end{cases}
$$

有n顶帽子，$P(X_i = 1) = \frac{1}{n}$

$E[X_i] = 1\cdot\frac{1}{n}+0\cdot(1-\frac{1}{n}) = \frac{1}{n}$

那么此处$X_i$之间是独立的吗？

并不是。想象一下如果一个人拿走了另一个的帽子，那被拿帽子的那个人就永远拿不到自己的帽子了。

但是这并不影响我们运用多变量期望的线性性质，因为**多变量期望的线性性质和随机变量的独立性无关**（证明中并没有用到独立性）

所以

$$
E[X] = E[\sum_{i=1}^n X_i] = \sum_{i=1}^n E[X_i] = n\frac{1}{n} = 1
$$

那么方差呢？就不能用二项分布例子中的方法了

因此，方差的计算需要用到其与期望之间的关系

$$
Var(X) = E[X^2]-E[X]^2\\
=E[(\sum_{i=1}^n X_i)^2] - 1\\
= E[\sum_{i=1}^n X_i^2+\sum_{i=1}^n\sum_{j=1}^n X_iX_j]-1\\
=\sum_{i=1}^n E[X_i^2] + \sum_{i,j,i\neq j} E[X_iX_j] - 1
$$

对于$\sum_{i=1}^n E[X_i^2]$，因为$X_i^2$和$X_i$一样，所以期望一样，该式就等于$n\cdot\frac{1}{n} = 1$

而对于$E[X_iX_j]$，我们知道，只有在$X_i,X_j$都为1，即两个人都是欧皇的时候，$X_iX_j$才能为非0，所以期望只需要计算$X_i,X_j$都为1的概率乘1即可，i.e.

$$
E[X_iX_j] = P(X_iX_j = 1)\cdot1+P(X_iX_j=0)\cdot0 = P(X_iX_j = 1)
$$

而

$$
P(X_iX_j = 1) = \frac{1}{n}\cdot\frac{1}{n-1}
$$

因为第一个人拿了帽子之后，第二个就只能在n-1顶帽子中选择

而这样的情况一共有$n^2-n$种，在$n^2$种情况中去除$i=j$的n种

综上

$$
Var(X) = 1+(n^2-n)\frac{1}{n(n-1)}-1 = 1+1-1=1
$$

和期望一样。事实上，这不是巧合 ~~一切都是命运石之门的选择！~~ 具体原因，在之后的几章会做讲解



参考资料：MIT6.041公开课程及讲义