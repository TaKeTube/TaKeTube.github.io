---
title: MIT 6.041 Probabilistic Systems Analysis and Applied Probability - 5.Discrete Random Variables I
layout: post
categories: [Probability, Math, MIT 6.041 Probabilistic Systems Analysis and Applied Probability]
image: /assets/img/MIT6.041/cover.jpg
description: "MIT 6.041 5.Discrete Random Variables I"
customexcerpt: "Notes for MIT opencourse MIT 6.041 Probabilistic Systems Analysis and Applied Probability"
---

* 
{:toc}



## 5.Discrete Random Variables I 离散随机变量（一）

试想掷骰子的实验，我们通常会说，投到3的概率是多少，投到6的概率是多少云云，并且在高中我们学过数学期望，会说我们投到数字的期望是多少... 那么这个和概率有联系的数字是什么呢？为了更好地数学化地讲清楚这一套东西，并且方便研究，就需要引入**随机变量 Random Variables**这个概念

#### Random Variables 随机变量

**随机变量 Random Variables 是一个从样本空间$\Omega$到实数域上的映射 i.e. $X:\Omega\rightarrow \mathbb{R}$**

这里有个很重要的概念，就是**随机变量事实上是一个函数而不是一个值**。对于每个样本空间中的结果，我们给它赋上一个实数值，结果与实数值之间有特定的联系，那么表示这个联系的方式就是映射。那么为什么要把这个映射叫做随机变量呢？看起来没什么联系？思考变量的含义，就是变化的量，我用一个x代替原本会出现在此处的值，而这个x会随着我选择的结果而变化。这么一解释，把这个映射叫做随机变量也就合情合理了。

拿掷骰子的实验为例

- 实验的结果是“掷骰子掷出了n”，那么我们可以把“掷骰子掷出了n”作为输入，这个n作为随机变量的输出

**一个样本空间，可以有多个随机变量**，因为到$\mathbb{R}$上的映射无穷无尽，想怎么来就怎么来。不过一般选择的都是有意义的随机变量，比如说样本空间是一个学校的学生，我们可以选择他们的身高作为随机变量，也可以选择他们的体重作为随机变量。

**随机变量的函数也是随机变量**。因为随机变量的函数值是由随机变量值确定的，而随机变量值是由样本空间的一个元素确定的，所以随机变量的函数也是一个映射。举一个例子，样本空间是一个学校的学生，他们的身高H是随机变量，2.5倍的身高2.5H同样也是随机变量。这里为了方便理解，把随机变量X当作了一个量，f(X)称之为其的一个函数，但是这里描述的函数，如果用数学语言准确确地说，其实是一个 由 $\Omega\rightarrow\mathbb{R}$上所有映射所构成的空间 到其本身的映射。

随机变量可以是**离散 discrete** 的，也可以是**连续 continues** 的。这取决于其值域是连续的还是离散的。用数学语言来讲，离散就是随机变量的值域是有限集或可数集，而连续则对应的不可数集。

- 离散的例子：掷骰子投出的数，值域是$\{1,2,3,4,5,6\}$，有限

- 连续的例子：一个学校里学生的身高，值域是身高的集合，可以无限精确，是不可数集

我们用大写的X表示随机变量，用小写的x表示该随机变量的数值。这里X是一个函数，而x是一个实数，这是需要弄清楚的概念。



#### Probability mass function (PMF) 概率质量函数

为了研究一个随机变量某个数值出现的可能性，e.g. 投到6的可能性，需要引入**概率质量函数 probability mass function (PMF)** 的概念

一个离散随机变量X的**概率质量函数 Probability mass function (PMF)**是一个由离散随机变量值域到$[0,1]$上的映射，用$p_X(x)$表示，且满足$p_X(x) = P({X = x})$，用以表示随机变量为某个值时的概率

注意这里的表示，为了简化，把随机变量当作了一个量，事实上准确地写应该是$p_X(x) = P(\{\omega\|\omega\in \Omega,\ X(\omega) = x\})$

**概率质量函数函数 PMF** 用于表示**离散随机变量**的概率关系，而**概率密度函数 PDF**用于表示**连续随机变量**的概率关系。这里先讨论PMF。

几个性质：

- $p_X(x)\geq 0$ 既然是概率，肯定非负
- $\sum_x p_X(x) = 1$  一个样本空间的结果对应一个随机变量值对应一个概率，PMF的定义域是所有的随机变量值，而随机变量的定义域是整个样本空间，所以PMF把样本空间“穷举”了，概率之和必定是1

 计算PMF的值很简单，第一步先找出使得X = x的所有样本空间的结果，再把这些结果的概率相加即可



#### Geometry PMF 几何概率质量函数 

若$X =$ 投硬币实验中，第一次投到正面所需的次数

假设投到正面的概率$P(H) = p$

那么$p_X(k) = P(X=k) = P(TT\cdots TH) = (1-p)^{k-1}p\quad\quad k=1,2,...$

我们称这样的概率质量函数为**几何PMF**，因为概率随k增大以几何倍数缩小

画出来大概是这样

<img src="\assets\img\MIT6.041\5\1.png" alt="1" style="zoom:60%;" />



#### Binomial PMF 二项概率质量函数

若$X =$ n次投硬币实验中投到的正面的数量

假设投到正面的概率$P(H) = p$

那么$p_X(k) = \binom{n}{k}p^k(1-p)^{n-k}\quad\quad k = 0,1,...,n$

把这个PMF的图像画出来大致是长这样的：

<img src="\assets\img\MIT6.041\5\2.png" alt="2" style="zoom:60%;" />

我们会发现随者n的增多，它越来越像正态分布的钟形曲线。这其实不是偶然，在之后的课中会讨论。



#### Expectation 期望

期望事实上就是随机变量值的加权平均，一个随机变量值的概率大一些，那么其出现的可能性就大一些，给予其的比率就高一些。

一个随机变量的期望$E[X]$满足

$$
E[X] = \sum_x xp_X(x)
$$

从另一个角度理解，期望就是PMF的重心

**期望的性质**

- 若$X$是一个随机变量，$Y = g(X)$

  计算$E[Y]$有两种方法

  - $E[Y] = \sum_yyp_Y(y)$ 
    - 先把$Y$的PMF找出来(每一个Y的值 找对应的所有样本空间里的结果)，再按定义计算，复杂
  - $E[Y] = \sum_xg(x)p_X(x)$
    - 直接通过X计算。假设有多个X对应一个Y的值，那么把这些X的概率相加就是该Y值的概率，所以此方法是可行的。

  这里需要注意的是，**随机变量的函数的期望一般不等于随机变量期望的函数**，i.e. $E[g(X)]\neq g(E[X])$

若$\alpha,\beta$是常数

- $E[\alpha] = \alpha$

  - 一个常数是一个特殊的随机变量（常数函数），意味着无论结果如何，对应的值都是$\alpha$, 自然其加权平均值就是$\alpha$

  - 证明

    $$
    E[\alpha] = \sum_x \alpha p_X(x) = \alpha\sum_xp_X(x) = \alpha
    $$

- $E[\alpha X] = \alpha E[X]$

  - 不难理解，若每个值都放大了$\alpha$倍，平均值也会放大$\alpha$倍

  - 证明:

    $$
    E[\alpha X] = \sum_x \alpha xp_X(x) = \alpha\sum_x xp_X(x) = \alpha E[X]
    $$

- $E[\alpha X+\beta] = \alpha E[X]+\beta$

  - 也不难理解，若每个值偏移了$\beta$，平均值也会偏移$\beta$

  - 证明:

    $$
    E[\alpha X+\beta] = \sum_x (\alpha x+\beta)p_X(x) = \alpha\sum_x xp_X(x) + \beta\sum_xp_X(x)= \alpha E[X]+\beta
    $$

由此可以看出，期望是一种**线性运算**

且**对于线性函数，随机变量的函数的期望等于随机变量期望的函数**，i.e. $E[g(X)] = g(E[X])\text{ for linear g} $



#### Variance 方差

对于一个随机变量$X$

我们称

$$
var(X) = E[(X-E[X])^2]
$$

为其**方差 variance**

其意义是一组随机变量的**分散程度**，方差大说明随机变量较为分散，方差小说明随机变量较为集中

从定义的意义上，它计算的是所有可能的随机变量值到平均值距离的平方的平均，平方说明其只关心距离，当随机变量越分散，距离平均值远的随机变量值的权重就越大，方差就越大

**方差的性质**

- $var(X) = E[(X-E[X])^2] = E[X^2]-(E[X])^2$

  - 证明:

    $$
    E[(X-E[X])^2] = \sum_x(x-E[X])^2p_X(x) \\= \sum_x(x^2-2xE[X]+E[X]^2)p_X(x)\\
    = \sum_xx^2p_X(x) - 2E[X]\sum_xxp_X(x) + E[X]^2\sum_xp_X(x)\\
    =E[X^2] - 2E[X]E[X] + E[X]^2\cdot 1\\
    =E[X^2]-E[X]^2
    $$

- $var(X)\geq 0$

  - 显然，因为计算的是距离平方的期望

- $var(\alpha X+\beta) = \alpha^2 var(X)$

  - 偏移量$\beta$ 对于方差不会有影响，因为方差关心的是相对的距离，偏移并不会改变相对距离

  - $\alpha$会使得方差变为2倍是因为，每个随机变量以及加权平均（期望）都放大了$\alpha$倍，距离的平方就放大了$\alpha^2$倍，所以距离平方的加权平均也放大了$\alpha$倍

  - 证明:

    $$
    var(\alpha X+\beta) = E[(\alpha X+\beta)^2]-E[\alpha X+\beta]^2\\
    =E[\alpha^2X^2+2\alpha\beta X+\beta^2] - (\alpha E[X]+\beta)^2\\
    =\alpha^2E[X^2]+2\alpha\beta E[X] + \beta^2 - \alpha^2E[X]^2-2\alpha\beta E[X]-\beta^2\\
    =\alpha^2(E[X^2]-E[X]^2)\\
    =\alpha^2 var(X)
    $$



参考资料：MIT6.041公开课程及讲义